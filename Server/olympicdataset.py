# -*- coding: utf-8 -*-
"""olympicDataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P_Cci_A5Z_xzGpOdMcR9Aenvrqk1P9Zx

# Şube: 1.A  , Ad: Ehsanullah, SoyAd: Shahriary, Numara: B191210563 Mail: ehsanullah.shahriary2@ogr.sakarya.edu.tr

# Proje adı: Tokyo 2020 Olympic Medals

# Proje Amacı:  
**burada bu çalışmada, olimpiyat madalyası kazananları tahmin ediliyor. Altın, Gümüş ve Bronz madalya sayısı da farklı makine öğrenimi modelleri kullanılarak tahmin ediliyor. Ve bir sonraki Olimpiyatlarda ilk 10 ülkenin sırasını veya performansını tahmin ediyor.**


# Verinin açıklanması:
**kullanan veri 93 satır ve 6 sütündan oluşmakta. değişkenler ise Country, Gold Medal, Silver Medal, Bronze Medal, Total and Ranked By Total bunlar dir.**

# Kullanan Büyük veri platformları: 
**Projede spark platformu ve farklı python kütüphaneler kullanılmıştır. Model için sample K-Fold ve Sk-learn Linear Regration kullandı.**

**Python da: numpy, pandas, matplotlib, seaborn, StandardScaler, statsmodels.formula.api.**


# Projenin açıklaması:
**Olimpiyat oyunları, yaz ve kış oyunları olmak üzere dört yılda bir düzenleniyor ve her olimpiyat oyununa farklı bir ülke ev sahipliği yapıyor. Her spor müsabakasında kazananlara 1.ye Altın, 2.ye Gümüş ve 3.ye bronz madalya verilir. burada bu çalışmada, olimpiyat madalyası kazananları tahmin ediliyor. Altın, Gümüş ve Bronz madalya sayısı da farklı makine öğrenimi modelleri kullanılarak tahmin ediliyor. Ve bir sonraki Olimpiyatlarda ilk 10 ülkenin sırasını veya performansını tahmin ediyor. Bu proje, kullanıcıların bu uygulamayı kullanarak bahis oyunlarında yardımcı olacak, kullanıcı Olimpiyat oyunlarının doğru galibini konuk edebilir ve bahsi kazanabilir.**

# Projenin çıktıları:

## **Setup**
"""

import math
import pandas as pd
import numpy as np 
import pickle
import os
# import wordcloud
# from wordcloud import WordCloud, STOPWORDS
from pandas import DataFrame
from sklearn.cluster import KMeans
from sklearn import preprocessing
# import matplotlib.pyplot as plt
# from matplotlib.colors import ListedColormap


# from google.colab import drive
# drive.mount('/content/drive')

# path='/content/drive/MyDrive/CSV_file/Tokyo Medals 2021.csv'
# df = pd.read_csv(path)
# df.describe

df= pd.read_csv("./Server/Tokyo Medals 2021.csv")


"""# Explore datasets



"""

df.head(10)




# #Second way....
# x = df.drop('Rank By Total', axis=1)
# y = df['Rank By Total']
# # Split the data into training and testing sets
# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 42)

# x.shape, y.shape

"""# **Splitting the data in to trianingData and testData**"""

# Using Skicit-learn to split data into training and testing sets
from sklearn.model_selection import train_test_split


x = df.drop('Rank By Total', axis=1)
y = df['Rank By Total']
# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 42)

print('Training Features Shape:', x_train.shape)
print('Training Labels Shape:', y_train.shape)
print('Testing Features Shape:', x_test.shape)
print('Testing Labels Shape:', y_test.shape)

print('Training Features Shape:', x_train.shape)
print('Training Labels Shape:', y_train.shape)
print('Testing Features Shape:', x_test.shape)
print('Testing Labels Shape:', y_test.shape)

#x

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ['Country']
one_hot = OneHotEncoder()
transformer = ColumnTransformer([("one_hot", one_hot,categorical_features)],remainder="passthrough")

transformed_x = transformer.fit_transform(x)


"""Messing Vlaue"""

from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder

# Create an imputer to replace missing values
imputer = SimpleImputer(strategy='most_frequent')

# Create a one-hot encoder to encode categorical variables
encoder = OneHotEncoder()

# Create the RandomForestRegressor
model = RandomForestRegressor()

# Create a pipeline to handle preprocessing and model training
pipeline = Pipeline(steps=[('encoder', encoder), ('imputer', imputer), ('model', model)])

# Fit the pipeline to your data
pipeline.fit(x, y)






# re-fit the model...
#build machine learning model
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier
#required for parameter tuning
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

##model = RandomForestRegressor()
np.random.seed(42)

# Split x,y...
#x_train, x_test, y_train, y_test = train_test_split(transformed_x, y, test_size= 0.49) #0.25,   0.02 , 45     49**

x_train, x_test, y_train, y_test = train_test_split(transformed_x, y, test_size= 0.5)
# Instantiate model with 1000 decision trees
RandomModel = RandomForestRegressor(n_estimators = 2600, random_state =3000)#42 ,4,       **2500   *3000**
grid = {"n_estimators": [10, 100, 200, 500, 1000, 1200],
"max_depth": [None, 5, 10, 20, 30],
"max_features": ["auto", "sqrt"],
"min_samples_split": [2,4,6],
"min_samples_leaf": [1, 2, 4]}


#Fit the model to the data(training the machine learning the model has)
RandomModel.fit(x_train, y_train)  #RandomModel.fit

"""# **Builging and Training The Model**"""

#build machine learning model
from sklearn.ensemble import RandomForestRegressor
#required for parameter tuning
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

##model = RandomForestRegressor()
np.random.seed(42)
# Split data after converting string to integer...
x_train, x_test, y_train, y_test = train_test_split(transformed_x, y, test_size= 0.5)

# Instantiate model with 1000 decision trees
model = RandomForestRegressor(n_estimators = 2500, random_state =3000)  #RandomModel
grid = {"n_estimators": [10, 100, 200, 500, 1000, 1200],
"max_depth": [None, 5, 10, 20, 30],
"max_features": ["auto", "sqrt"],
"min_samples_split": [2,4,6],
"min_samples_leaf": [1, 2, 4]}


#Fit the model 
model.fit(x_train, y_train)#RandomModel

#RandomModel.score(x_test, y_test)
model.score(x_test, y_test)

#RandomModel.predict(x_test)
model.predict(x_test)

#Compare prediction to truth labels to evaluate the model...
# y_preds =RandomModel.predict(x_test)
# np.mean(y_preds == y_test)

y_preds =model.predict(x_test)
np.mean(y_preds == y_test)

# RandomModel.score(x_test, y_test)
model.score(x_test, y_test)

from sklearn.metrics import accuracy_score
#accuracy_score(y_test, y_preds)

df=pd.DataFrame({'Actual':y_test, 'Predicted':y_preds})

from sklearn import metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_preds))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_preds))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_preds)))

# Calculate the absolute errors
errors = abs(y_preds - y_test)
# Print out the mean absolute error (mae)
print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')

# Calculate mean absolute percentage error (MAPE)
mape = 100 * (errors / y_test)
# Calculate and display accuracy
accuracy = 100 - np.mean(mape)
print('Accuracy:', round(accuracy, 2), '%.')

# Save the model as a pickle file
# Assuming 'model' is your trained ML model
with open('random_forest_model3.pkl', 'wb') as file:
    pickle.dump(model, file)

