# -*- coding: utf-8 -*-
"""olympicDataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P_Cci_A5Z_xzGpOdMcR9Aenvrqk1P9Zx

# Şube: 1.A  , Ad: Ehsanullah, SoyAd: Shahriary, Numara: B191210563 Mail: ehsanullah.shahriary2@ogr.sakarya.edu.tr

# Proje adı: Tokyo 2020 Olympic Medals

# Proje Amacı:  
**burada bu çalışmada, olimpiyat madalyası kazananları tahmin ediliyor. Altın, Gümüş ve Bronz madalya sayısı da farklı makine öğrenimi modelleri kullanılarak tahmin ediliyor. Ve bir sonraki Olimpiyatlarda ilk 10 ülkenin sırasını veya performansını tahmin ediyor.**


# Verinin açıklanması:
**kullanan veri 93 satır ve 6 sütündan oluşmakta. değişkenler ise Country, Gold Medal, Silver Medal, Bronze Medal, Total and Ranked By Total bunlar dir.**

# Kullanan Büyük veri platformları: 
**Projede spark platformu ve farklı python kütüphaneler kullanılmıştır. Model için sample K-Fold ve Sk-learn Linear Regration kullandı.**

**Python da: numpy, pandas, matplotlib, seaborn, StandardScaler, statsmodels.formula.api.**


# Projenin açıklaması:
**Olimpiyat oyunları, yaz ve kış oyunları olmak üzere dört yılda bir düzenleniyor ve her olimpiyat oyununa farklı bir ülke ev sahipliği yapıyor. Her spor müsabakasında kazananlara 1.ye Altın, 2.ye Gümüş ve 3.ye bronz madalya verilir. burada bu çalışmada, olimpiyat madalyası kazananları tahmin ediliyor. Altın, Gümüş ve Bronz madalya sayısı da farklı makine öğrenimi modelleri kullanılarak tahmin ediliyor. Ve bir sonraki Olimpiyatlarda ilk 10 ülkenin sırasını veya performansını tahmin ediyor. Bu proje, kullanıcıların bu uygulamayı kullanarak bahis oyunlarında yardımcı olacak, kullanıcı Olimpiyat oyunlarının doğru galibini konuk edebilir ve bahsi kazanabilir.**

# Projenin çıktıları:

## **Setup**
"""

import math
import pandas as pd
import numpy as np 
import seaborn as sns
import os
import wordcloud
from wordcloud import WordCloud, STOPWORDS
from pandas import DataFrame
from sklearn.cluster import KMeans
from sklearn import preprocessing
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

!pip install pyspark py4j

from google.colab import drive
drive.mount('/content/drive')

path='/content/drive/MyDrive/CSV_file/Tokyo Medals 2021.csv'
df = pd.read_csv(path)
df.describe

os.getcwd()

"""# Explore datasets



"""

df.head(10)

df.shape

df.isnull().sum()

"""**- Check null values**"""

nan_values = df.isna()
nan_columns= nan_values.any()
nan_columns

df.info

df.columns

"""# **Data Analysis...**

**- Leading Contries...**
"""

leading= df.sort_values("Rank By Total", ascending = True)
leading

"""**- Gold Medal**"""

goldLeading = df.sort_values("Gold Medal", ascending = False)
goldLeading

"""**- Silver Medals**"""

silverLeading = df.sort_values("Silver Medal", ascending = False)
silverLeading

"""**- Bronze Medal**"""

bronzeLeading = df.sort_values("Bronze Medal", ascending = False)
bronzeLeading

"""# **Which contries have more than 15 Gold medals**"""

filter=df["Gold Medal"]>15
filtringData=df[filter]
filtringData

"""# **Filter for spacific country**"""

filterSpacificCountry=df["Country"].str.contains("Turkey")
filtringData2=df[filterSpacificCountry]
filtringData2

"""#**data Visualization**"""

plt.style.use("ggplot")

plt.figure(figsize=(25,14))
sns.barplot(x='Country', y='Gold Medal', data=df)
plt.plot(df.Country,df['Gold Medal'], marker='s', c='g', ls='-', lw= 3, ms=5, mew=1, mec='navy');
plt.tick_params(axis='x',which='major',labelsize=13,rotation=90)
plt.tick_params(axis='y',which='major',labelsize=20)
plt.title("Gold Medal Won By Each Countries",fontsize=30,pad=20)
plt.xlabel("Countries",fontsize=25)
plt.ylabel("Gold Medals Won",fontsize=25)
plt.legend(['Countries',' Gold Medals Won']);
plt.grid(color='g',linestyle='-',linewidth=0.3,alpha=0.5)

plt.style.use("ggplot")

plt.figure(figsize=(20,14))
sns.barplot(x='Country', y='Silver Medal', data= df)
plt.plot(df.Country, df["Silver Medal"], marker="s", color="yellow", ls="-", linewidth=3, mew=1, mec="navy");
plt.tick_params(axis='x', which="major", labelsize=13, rotation=90)
plt.tick_params(axis="y", which= "major", labelsize=13, rotation=90)
plt.title("Silver Medals Won By Each Country", fontsize=20)
plt.xlabel("Countries", fontsize=20)
plt.ylabel("Silver Medals", fontsize=20)
plt.grid(color="g", linestyle="-", linewidth=.5, alpha=.3)

plt.style.use("ggplot")

plt.figure(figsize=(20,14))
sns.barplot(x='Country', y='Bronze Medal', data= df)
plt.plot(df.Country, df["Bronze Medal"], marker="x", color="b", ls="-", linewidth=3, ms=5, mew=1, mec="navy");
plt.tick_params(axis='x', which="major", labelsize=13, rotation=90)
plt.tick_params(axis="y", which= "major", labelsize=20)
plt.title("Bronze Medals Won By Each Country", fontsize=20)
plt.xlabel("Countries", fontsize=20)
plt.ylabel("Bronze Medals won", fontsize=20)
plt.legend(["Countries", "Bronze Medals Won"])
plt.grid(color="g", linestyle="-", linewidth=.5, alpha=.3)

plt.figure(figsize=(20,14))
plt.plot(df.Country,df["Gold Medal"],marker='s',c='r',ls='-',lw=1,ms=4,mew=1,mec='navy');
plt.plot(df.Country,df["Silver Medal"],marker='s',c='b',ls='-',lw=1,ms=4,mew=1,mec='navy');
plt.plot(df.Country,df["Bronze Medal"],marker='s',c='g',ls='-',lw=1,ms=4,mew=1,mec='navy');
plt.tick_params(axis='x',which='major',labelsize=13,rotation=90)
plt.tick_params(axis='y',which='major',labelsize=20)
plt.title("Total Medals Won By Each Countries",fontsize=30,pad=20)
plt.xlabel("Countries",fontsize=25)
plt.ylabel(" Medals Count",fontsize=25)
plt.legend([' Gold Medals Won','Silver Medals Won','Bronze Medals Won']);
plt.grid(color='g',linestyle='-',linewidth=0.3,alpha=0.5)

"""# **Who are the top 10 countries by total medals in tokyo 2020 olympic that has the most gold or silver or bronze medals? how many are their medals??**

**- Top 10 contries that has the most gold medals in Tokyo olympic 2020**
"""

plt.figure(figsize = (10, 8))
plt.title('Top 10 countries that has the most gold medals in Tokyo Olympic 2020', size = 13)
bplot = sns.barplot(x = df['Gold Medal'], y = df['Country'], order = df.sort_values('Gold Medal', ascending = False)['Country'].iloc[:10])
for p in bplot.patches:
    bplot.annotate("%.0f" % p.get_width(), (p.get_width(), p.get_y() + p.get_height() / 2), (5, 0), 
                   textcoords = 'offset points', ha = 'left', va = 'center')

plt.xlabel('Gold Medals', fontsize = 12)
plt.ylabel('Country', fontsize = 12)
plt.show()

"""**- Top 10 countries that has the most silver medals in Tolkyo Olympic 2020**"""

plt.figure(figsize = (10, 8))
plt.title('Top 10 countries that has the most silver medals in Tokyo Olympic 2020', size = 13)
bplot = sns.barplot(x = df['Gold Medal'], y = df['Country'], order = df.sort_values('Gold Medal', ascending = False)['Country'].iloc[:10])
for p in bplot.patches:
    bplot.annotate("%.0f" % p.get_width(), (p.get_width(), p.get_y() + p.get_height() / 2), (5, 0), 
                   textcoords = 'offset points', ha = 'left', va = 'center')

plt.xlabel('Gold Medals', fontsize = 12)
plt.ylabel('Country', fontsize = 12)
plt.show()

"""**- Top 10 countries that has the most bronze medals in Tokyo Olympic 2020**"""

plt.figure(figsize = (10, 8))
plt.title('Top 10 countries that has the most bronze medals in Tokyo Olympic 2020', size = 13)
bplot = sns.barplot(x = df['Gold Medal'], y = df['Country'], order = df.sort_values('Gold Medal', ascending = False)['Country'].iloc[:10])
for p in bplot.patches:
    bplot.annotate("%.0f" % p.get_width(), (p.get_width(), p.get_y() + p.get_height() / 2), (5, 0), 
                   textcoords = 'offset points', ha = 'left', va = 'center')

plt.xlabel('Gold Medals', fontsize = 12)
plt.ylabel('Country', fontsize = 12)
plt.show()

"""**- Top 10 countries that has the most medals in Tokyo olympic 2020**"""

plt.figure(figsize = (10, 8))
plt.title('Top 10 countries that has the most medals in Tokyo Olympic 2020', size = 13)
bplot = sns.barplot(y = df['Country'], x = df['Total'], order = df.sort_values('Total', ascending = False)['Country'].iloc[:10])
for p in bplot.patches:
    bplot.annotate("%.0f" % p.get_width(), (p.get_width(), p.get_y() + p.get_height() / 2), (5, 0), 
                   textcoords = 'offset points', ha = 'left', va = 'center')
    
plt.xlabel('Total Medals', fontsize = 12)
plt.ylabel('Country', fontsize = 12)
plt.xticks(rotation = 90)
plt.show()

"""# **Split data to X and Y array**"""

#x= df[['Country','Gold Medal','Silver Medal','Bronze Medal','Total', 'Rank By Total']]

#from sklearn.model_selection import train_test_split
#y=np.array(df['Rank By Total'])
# Remove the labels from the features
# axis 1 refers to the columns
#x= x.drop('actual', axis = 1)
# Saving feature names for later use
#x_list = list(x.columns)
# Convert to numpy array
#x = np.array(x)


# Firsst one......
#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)
#x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, test_size=0.3, random_state=100np_array = df.to_numpy()

# Using Skicit-learn to split data into training and testing sets
from sklearn.model_selection import train_test_split

#x= df[['Country','Gold Medal','Silver Medal','Bronze Medal','Total', 'Rank By Total']]



# First Way...
#x = df.iloc[:, :-1]
#y = df.iloc[:, -1]

#...*****************************************************

# Conver string to num
#x = x.apply(pd.to_numeric, errors='coerce')
#y = y.apply(pd.to_numeric, errors='coerce')


#...*****************************************************


# Remove the labels from the features
# axis 1 refers to the columns
#x= df.drop('Rank By Total', axis = 1)

# Labels are the values we want to predict
#y = np.array(df['Rank By Total'])


# Saving feature names for later use
#x_list = list(x.columns) 



# Convert to numpy array
#x = np.array(x)

#...*****************************************************


#Second way....
x = df.drop('Rank By Total', axis=1)
y = df['Rank By Total']
# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 42)

x.shape, y.shape

"""# **Splitting the data in to trianingData and testData**"""

# Using Skicit-learn to split data into training and testing sets
from sklearn.model_selection import train_test_split


x = df.drop('Rank By Total', axis=1)
y = df['Rank By Total']
# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 42)

print('Training Features Shape:', x_train.shape)
print('Training Labels Shape:', y_train.shape)
print('Testing Features Shape:', x_test.shape)
print('Testing Labels Shape:', y_test.shape)

print('Training Features Shape:', x_train.shape)
print('Training Labels Shape:', y_train.shape)
print('Testing Features Shape:', x_test.shape)
print('Testing Labels Shape:', y_test.shape)

x

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ['Country']
one_hot = OneHotEncoder()
transformer = ColumnTransformer([("one_hot", one_hot,categorical_features)],remainder="passthrough")

transformed_x = transformer.fit_transform(x)
transformed_x

pd.DataFrame(transformed_x)

dummies = pd.get_dummies(df[["Country"]])
dummies

"""Messing Vlaue"""

from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline

# Create an imputer to replace NaN values
imputer = SimpleImputer(strategy='mean')

# Create the RandomForestRegressor
model = RandomForestRegressor()

# Create a pipeline to handle preprocessing and model training
pipeline = Pipeline(steps=[('imputer', imputer), ('model', model)])

# Fit the pipeline to your data
pipeline.fit(transformed_x, y)

model.predict(x_test)

model.score(x_test, y_test)



# re-fit the model...
#build machine learning model
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier
#required for parameter tuning
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

##model = RandomForestRegressor()
np.random.seed(42)

# Split x,y...
#x_train, x_test, y_train, y_test = train_test_split(transformed_x, y, test_size= 0.49) #0.25,   0.02 , 45     49**

x_train, x_test, y_train, y_test = train_test_split(transformed_x, y, test_size= 0.5)
# Instantiate model with 1000 decision trees
RandomModel = RandomForestRegressor(n_estimators = 2600, random_state =3000)#42 ,4,       **2500   *3000**
grid = {"n_estimators": [10, 100, 200, 500, 1000, 1200],
"max_depth": [None, 5, 10, 20, 30],
"max_features": ["auto", "sqrt"],
"min_samples_split": [2,4,6],
"min_samples_leaf": [1, 2, 4]}
#RandomModel = RandomForestRegressor(n_estimators = 1000, random_state =500)#42 ,4, 95     *100**   500***
#RandomModel = RandomForestClassifier(n_estimators=1000, criterion='gini', max_depth=None,min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None,bootstrap=True, oob_score=False, n_jobs=1, random_state=100, verbose=0, warm_start=False,class_weight=None)
# Second way.... 86.98

#grid = {"n_estimators": [10, 100, 200, 500, 1000, 1200],
#"max_depth": [None, 5, 10, 20, 30],
#"max_features": ["auto", "sqrt"],
#"min_samples_split": [2,4,6],
#"min_samples_leaf": [1, 2, 4]}
#np.random.seed(42)
#clf = RandomForestClassifier(n_jobs=-1)
#RandomModel = RandomizedSearchCV(estimator=clf,
#param_distributions = grid,
#n_iter=30,
#cv = 5,
#verbose = 2)
#andomModel.fit(x_train, y_train);
#accuracy_score(y_test, RandomModel.predict(x_test))

# Third way.... 


#Fit the model to the data(training the machine learning the model has)
model.fit(x_train, y_train)  #RandomModel.fit

"""# **Builging and Training The Model**"""

#build machine learning model
from sklearn.ensemble import RandomForestRegressor
#required for parameter tuning
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

##model = RandomForestRegressor()
np.random.seed(42)
# Split data after converting string to integer...
x_train, x_test, y_train, y_test = train_test_split(transformed_x, y, test_size= 0.5)

# Instantiate model with 1000 decision trees
model = RandomForestRegressor(n_estimators = 2500, random_state =3000)  #RandomModel
grid = {"n_estimators": [10, 100, 200, 500, 1000, 1200],
"max_depth": [None, 5, 10, 20, 30],
"max_features": ["auto", "sqrt"],
"min_samples_split": [2,4,6],
"min_samples_leaf": [1, 2, 4]}


#Fit the model 
model.fit(x_train, y_train)#RandomModel

#RandomModel.score(x_test, y_test)
model.score(x_test, y_test)

#RandomModel.predict(x_test)
model.predict(x_test)

#Compare prediction to truth labels to evaluate the model...
# y_preds =RandomModel.predict(x_test)
# np.mean(y_preds == y_test)

y_preds =model.predict(x_test)
np.mean(y_preds == y_test)

# RandomModel.score(x_test, y_test)
model.score(x_test, y_test)

from sklearn.metrics import accuracy_score
#accuracy_score(y_test, y_preds)

df=pd.DataFrame({'Actual':y_test, 'Predicted':y_preds})
df

from sklearn import metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_preds))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_preds))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_preds)))

# Calculate the absolute errors
errors = abs(y_preds - y_test)
# Print out the mean absolute error (mae)
print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')

# Calculate mean absolute percentage error (MAPE)
mape = 100 * (errors / y_test)
# Calculate and display accuracy
accuracy = 100 - np.mean(mape)
print('Accuracy:', round(accuracy, 2), '%.')

"""# Saved and laod model as pkl file"""

import pickle
import shutil
# Save the model as a pickle file
with open('random_forest_model_2.pkl', 'wb') as file:
  pickle.dump(RandomModel, file)

#Move the pickle file to Google Drive
shutil.move('random_forest_model.pkl', '/content/random_forest_model.pkl')

# # Load the model from the pickle file
# with open('/content/random_forest_model_2.pkl', 'rb') as file:
#     loaded_model = pickle.load(file)

# loaded_model.predict(x_test)

with open('/content/random_forest_model_2.pkl', 'rb') as file:
    loaded_model = pickle.load(file)

# Fit the loaded model to your training data
loaded_model.fit(x_train, y_train)

# Make predictions using the fitted model
loaded_model.predict(x_test)

"""### Converting Random forest model to Json format"""

!pip install joblib

import joblib

# Load the Random Forest model from the pickle file
model = joblib.load('/content/random_forest_model_2.pkl')

import json
import numpy as np
from sklearn.tree import _tree

# Load the Random Forest model from the pickle file
model = joblib.load('/content/random_forest_model_2.pkl')
# Fit the loaded model to your training data
model.fit(x_train, y_train)
# Convert a single DecisionTreeRegressor to a JSON-serializable dictionary
def tree_to_dict(tree):
    tree_dict = {
        'children_left': tree.children_left.tolist(),
        'children_right': tree.children_right.tolist(),
        'feature': tree.feature.tolist(),
        'threshold': tree.threshold.tolist(),
        'value': tree.value.tolist(),
        'impurity': tree.impurity.tolist(),
    }
    return tree_dict

# Convert model attributes to JSON-compatible format
model_json = {
    'estimators': [],
    'feature_importances': model.feature_importances_.tolist(),
    # Include other necessary model attributes
}

# Convert each DecisionTreeRegressor in the ensemble to dictionary format
for estimator in model.estimators_:
    estimator_dict = tree_to_dict(estimator.tree_)
    model_json['estimators'].append(estimator_dict)

# Save the model as a JSON file
with open('/content/random_forest_model_2.json', 'w') as file:
    json.dump(model_json, file)

"""## converting Random forest model ot TessorFolwLite

SECOND WAY...
"""

from sklearn.linear_model import LogisticRegression 
from sklearn.metrics import confusion_matrix, accuracy_score 
logistic_model = LogisticRegression()

x_train, x_test, y_train, y_test = train_test_split(transformed_x, y, test_size= 0.25)   #0.25

# Training the model on the training data and labels
logistic_model.fit(x_train, y_train)

# Using the model to predict the labels of the test data
y_pred = logistic_model.predict(x_test)
np.mean(y_pred == y_test)

#Evaluate the model
accuracy = accuracy_score(y_test,y_pred)*100
confusion_mat = confusion_matrix(y_test,y_pred)

#Printing the result...
print("Accuracy is",accuracy)
print("Confusion Matrix")
print(confusion_mat)

#Improving model...
logistic_Model = LogisticRegression(random_state=900) #1234
logistic_Model.fit(x_train, y_train)

y_predicted = logistic_Model.predict(x_test)
print(y_predicted)

#required to evaluate the result
from sklearn.metrics import classification_report
print("Classification Report is: \n",classification_report(y_test, y_predicted))

parameter_grid_logistic_regression = {
    'max_iter': [20, 50, 100, 200, 500, 1000],                      # Number of iterations 20 50 100 200 500 1000
    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],   # Algorithm to use for optimization
    'class_weight': ['balanced']                                    # Troubleshoot unbalanced data sampling
}

#required for grahical representation purpose
from sklearn.metrics import roc_curve, auc

#required for parameter tuning
from sklearn.model_selection import GridSearchCV

logistic_Model_grid = GridSearchCV(estimator=LogisticRegression(random_state=1234), param_grid=parameter_grid_logistic_regression, verbose=1, 
                    cv=10, n_jobs=-1)
 
logistic_Model_grid.fit(x_train, y_train)
 
print("Best score for the model after tuning is: ",logistic_Model_grid.best_score_)
print("Best parameters for the model is :",logistic_Model_grid.best_estimator_)

accuracy = accuracy_score(y_test,y_predicted)*100
#Printing the result...
print("Accuracy is",accuracy)

# from sklearn.linear_model import LinearRegression
# from sklearn.model_selection import KFold

# model = LinearRegression()

# # 2nd + 3rd param: shuffle data before splitting into folds
# kfold = KFold(n_splits=3, shuffle=True, random_state=1)
# model = 1
# # displaying indices for the rows that will be for training/testing
# for train, test in kfold.split(x):
#   print('Model #%d:' % model)
#  # print('train: %s, test: %s' % (x_train, y_test))
#   print('train: %s, test: %s' % (train, test))
#   model = model+1

